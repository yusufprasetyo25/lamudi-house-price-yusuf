{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Things to keep in mind while reading this consolidated code:\n",
    "1. This is consolidated code to understand the *workflow* of this project from start to end\n",
    "2. In this code, you will be able to follow all the *major steps*\n",
    "3. I will *skip the detailed data patching/cleaning*. There are invalid data from google maps (e.g. two entries in google maps for the same mall)\n",
    "4. I will *refer to a specific notebook that I used* and needed files (if any) when creating this project for every section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. *Nilai Jual Objek Pajak* (NJOP) download and parse\n",
    "\n",
    "**Needed file**:\n",
    "njop2021.pdf from [Pergub Nomor 17 Tahun 2021 Tentang Penetapan NJOP PBB-P2 Tahun 2021](https://bprd.jakarta.go.id/peraturan-perpajakan/unduh/pergub-nomor-17-tahun-2021-tentang-penetapan-njop-pbbp2-tahun-2021)\n",
    "\n",
    "**Relevant Notebook File**:\n",
    "transform_njop_to_csv.ipynb\n",
    "\n",
    "**Output files**:\n",
    "1. njop2021.pdf\n",
    "2. NJOP2021.csv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Download the NJOP File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://bprd.jakarta.go.id/peraturan-perpajakan/unduh/pergub-nomor-17-tahun-2021-tentang-penetapan-njop-pbbp2-tahun-2021'\n",
    "response = requests.get(url)\n",
    "with open('njop2021.pdf', 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Parse the NJOP File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "provinsi = []\n",
    "kota = []\n",
    "kecamatan = []\n",
    "kelurahan = []\n",
    "blk = []\n",
    "nama_jalan = []\n",
    "znt = []\n",
    "kelas_bumi = []\n",
    "min_njb = []\n",
    "sd = []\n",
    "max_njb = []\n",
    "ave_njb = []\n",
    "page_num = []\n",
    "\n",
    "reader = PdfReader(\"njop2021.pdf\")\n",
    "\n",
    "# parsing by pattern\n",
    "for num in range(4, 4204): # until page 4203\n",
    "    print(\"Now on page\", num)\n",
    "    page = reader.pages[num]\n",
    "    x = page.extract_text()\n",
    "    x = x.splitlines()\n",
    "    words = [word for word in x if not '$' in word]\n",
    "    for i in range(len(words)):\n",
    "        # every page contains only one provinsi, kota, kecamatan, and kelurahan\n",
    "        # next to every title is the information\n",
    "        if 'PROPINSI' in words[i-1]:\n",
    "            provinsi_page = words[i]\n",
    "        if 'KOTA/KAB' in words[i-1]:\n",
    "            kota_page = words[i]\n",
    "        if 'KECAMATAN' in words[i-1]:\n",
    "            kecamatan_page = words[i]\n",
    "        if 'KELURAHAN' in words[i-1]:\n",
    "            kelurahan_page = words[i]\n",
    "    for i in range(len(words)):\n",
    "        \"\"\"\n",
    "        Pattern:\n",
    "        1. 'BLK' contains 3 numbers\n",
    "        2. 'ZNT code' contains 2 letters\n",
    "        3. 'Kelas Bumi' contains 3 numbers\n",
    "        4. 'Sixth column' is 's/d' string\n",
    "        \"\"\"\n",
    "        # let's use last column as the ith place\n",
    "        if re.match(r\"\\d{3}\", x[i-7])\\\n",
    "        and re.match(r\"[A-Z]{2}\", x[i-5])\\\n",
    "        and re.match(r\"\\d{3}\", x[i-4])\\\n",
    "        and x[i-2] == 's/d':\n",
    "            provinsi.append(provinsi_page)\n",
    "            kota.append(kota_page)\n",
    "            kecamatan.append(kecamatan_page)\n",
    "            kelurahan.append(kelurahan_page)\n",
    "            blk.append(x[i-7])\n",
    "            nama_jalan.append(x[i-6])\n",
    "            znt.append(x[i-5])\n",
    "            kelas_bumi.append(x[i-4])\n",
    "            min_njb.append(x[i-3])\n",
    "            sd.append(x[i-2])\n",
    "            max_njb.append(x[i-1])\n",
    "            ave_njb.append(x[i])\n",
    "            page_num.append(num)\n",
    "\n",
    "# insert to a DataFrame\n",
    "df = pd.DataFrame(data={'provinsi': provinsi\n",
    "                  , 'kota': kota\n",
    "                  , 'kecamatan': kecamatan\n",
    "                  , 'kelurahan': kelurahan\n",
    "                  , 'blk': blk\n",
    "                  , 'nama_jalan': nama_jalan\n",
    "                  , 'znt': znt\n",
    "                  , 'kelas_bumi': kelas_bumi\n",
    "                  , 'min_njb': min_njb\n",
    "                  , 'sd': sd\n",
    "                  , 'max_njb': max_njb\n",
    "                  , 'ave_njb': ave_njb\n",
    "                  , 'page_num': page_num})\n",
    "\n",
    "# create CSV file from parsed NJOP\n",
    "df.to_csv('NJOP2021.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scrape from [Lamudi](https://www.lamudi.co.id/)\n",
    "\n",
    "**Relevant Notebook File**: scrape_lamudi.ipynb\n",
    "\n",
    "**Output file**: lamudi_house_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import math\n",
    "\n",
    "links = []\n",
    "data_price_s = []\n",
    "data_category_s = []\n",
    "data_subcategories_s = []\n",
    "data_bedrooms_s = []\n",
    "data_bathrooms_s = []\n",
    "data_building_size_s = []\n",
    "data_land_size_s = []\n",
    "data_furnished_s = []\n",
    "data_sku_s = []\n",
    "data_geo_point_s = []\n",
    "page_link_s = []\n",
    "parent_link_s = []\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.lamudi.co.id/jakarta/house/buy/\")\n",
    "click_expand = driver.find_element(By.XPATH, '//*[@id=\"js-crosslinkTopFilter\"]/a').click()\n",
    "sublinks = driver.find_elements(By.XPATH, '//*[@id=\"js-crosslinkTopFilter\"]/div/div/div/a')\n",
    "for tag in sublinks:\n",
    "    links.append(tag.get_attribute('href'))\n",
    "driver.quit()\n",
    "for link in links:\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(link)\n",
    "    page_end = 1 # initialize page_end with number 1 to be refined, only for looping purpose\n",
    "    page_iter = 1\n",
    "    while page_iter <= page_end:\n",
    "        info = driver.find_elements(By.CSS_SELECTOR, \".ListingCell-AllInfo.ListingUnit\")\n",
    "        info_a = driver.find_elements(By.CSS_SELECTOR, \".js-listing-link\")\n",
    "        try:\n",
    "            page_end = int(driver.find_element(By.CSS_SELECTOR, \"*[data-pagination-end]:last-child\").get_attribute('data-pagination-end'))\n",
    "        except:\n",
    "            pass\n",
    "        for i in range(len(info)):\n",
    "            data_price_s.append(info[i].get_attribute('data-price'))\n",
    "            data_category_s.append(info[i].get_attribute('data-category'))\n",
    "            data_subcategories_s.append(info[i].get_attribute('data-subcategories'))\n",
    "            data_bedrooms_s.append(info[i].get_attribute('data-bedrooms'))\n",
    "            data_bathrooms_s.append(info[i].get_attribute('data-bathrooms'))\n",
    "            data_building_size_s.append(info[i].get_attribute('data-building_size'))\n",
    "            data_land_size_s.append(info[i].get_attribute('data-land_size'))\n",
    "            data_furnished_s.append(info[i].get_attribute('data-furnished'))\n",
    "            data_sku_s.append(info[i].get_attribute('data-sku'))\n",
    "            data_geo_point_s.append(info[i].get_attribute('data-geo-point'))\n",
    "            page_link_s.append(info_a[i*2].get_attribute('href')) # js-listing-link class is written twice in the website\n",
    "            parent_link_s.append(link)\n",
    "        page_iter += 1\n",
    "        if page_end >= page_iter:\n",
    "            click_expand = driver.find_element(By.CSS_SELECTOR, '.next a[data-next-page]').click()\n",
    "            time.sleep(math.ceil(2.811 + np.random.normal(loc=0.0,scale=0.627), 0)) # delay so it doesn't break or startle the website\n",
    "    driver.quit()\n",
    "\n",
    "# insert to a DataFrame\n",
    "df = pd.DataFrame(data={'data_price_s': data_price_s,\n",
    "'data_category_s': data_category_s,\n",
    "'data_subcategories_s': data_subcategories_s,\n",
    "'data_bedrooms_s': data_bedrooms_s,\n",
    "'data_bathrooms_s': data_bathrooms_s,\n",
    "'data_building_size_s': data_building_size_s,\n",
    "'data_land_size_s': data_land_size_s,\n",
    "'data_furnished_s': data_furnished_s,\n",
    "'data_sku_s': data_sku_s,\n",
    "'data_geo_point_s': data_geo_point_s,\n",
    "'page_link_s': page_link_s,\n",
    "'parent_link_s': parent_link_s})\n",
    "\n",
    "# create CSV file from scraped website\n",
    "df.to_csv('lamudi_house_dataset.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Geocode from NJOP Files\n",
    "\n",
    "**Relevant Notebook Files**:\n",
    "1. NJOP2021_geocode.ipynb\n",
    "2. EDA_NJOP.ipynb\n",
    "\n",
    "**Output File**: jakarta_street_coordinate_patched.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim # https://nominatim.org/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "df = pd.read_csv('./NJOP2021.csv')\n",
    "df = df.drop(['Unnamed: 0', 'blk', 'znt', 'kelas_bumi', 'sd', 'page_num'], axis=1)\n",
    "\n",
    "cols = ['provinsi', 'kota', 'kecamatan', 'kelurahan']\n",
    "for col in cols:\n",
    "    df[col] = df[col].apply(lambda x: str(x).split(' - ')[1])\n",
    "\n",
    "# There are weird kecamatan and kelurahan from NJOP Files to be cleaned\n",
    "remove_spaces = [\n",
    "    'P U L O', 'C I K O K O', 'P A N C O R A N', 'R A G U N A N', 'S E L O N G', 'S E N A Y A N', 'K A R E T'\n",
    "    , 'G U N T U R', 'KALI BATA', 'B A N G K A', 'U L U J A M I', 'C I P U L I R', 'PAL MERIAM', 'KALI ANYAR'\n",
    "    , 'RAWA JATI', 'SETIA BUDI'\n",
    "]\n",
    "df['kelurahan'] = df['kelurahan'].apply(lambda x: x.replace(' ', '') if x in remove_spaces else x)\n",
    "df['kelurahan'] = df['kelurahan'].str.replace('KEBAYORAN LAMA UTR', 'KEBAYORAN LAMA UTARA').str.replace('KEBAYORAN LAMA SLT', 'KEBAYORAN LAMA SELATAN').str.replace('BALIMESTER', 'BALI MESTER')\n",
    "remove_spaces = ['C I L A N D A K', 'J A G A K A R S A', 'P A N C O R A N', 'T E B E T', 'SETIA BUDI']\n",
    "df['kecamatan'] = df['kecamatan'].apply(lambda x: x.replace(' ', '') if x in remove_spaces else x)\n",
    "df = df.drop_duplicates(ignore_index=True)\n",
    "\n",
    "dict = {}\n",
    "for idx in range(len(df)):\n",
    "    print('Now on index', idx)\n",
    "    try: # try except is needed since my internet connection is unstable\n",
    "        geolocator = Nominatim(user_agent=\"yusuf-application\")\n",
    "        location = geolocator.geocode(df.iloc[idx]['nama_jalan']+', '+df.iloc[idx]['kelurahan'], timeout=10)\n",
    "        if location:\n",
    "            dict[idx] = [location.latitude, location.longitude]\n",
    "    except:\n",
    "        idx -= 1\n",
    "        time.sleep(60) # wait for internet to reconnect\n",
    "for i in dict:\n",
    "    df['coordinate'].iloc[i] = dict[i]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "# create CSV file from street and coordinate\n",
    "df.to_csv('jakarta_street_coordinate_patched.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis (EDA) Lamudi\n",
    "\n",
    "**Input File**: lamudi_house_dataset.csv\n",
    "\n",
    "**Relevant Notebook File**: EDA_NJOP.ipynb\n",
    "\n",
    "**Output File**: lamudi_house_dataset_cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for processing dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import ceil, floor\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to display all the columns of the dataframe in the notebook\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('lamudi_house_dataset.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1) # from the creation of csv\n",
    "# remove duplicates\n",
    "df = df.drop_duplicates('data_sku_s', keep='first').reset_index(drop=True)\n",
    "print('Number of rows after dropping duplicate rows:', len(df))\n",
    "# remove incomplete important data\n",
    "df = df[ ~df['data_price_s'].isna() ].reset_index(drop=True)\n",
    "df = df[ ~df['data_geo_point_s'].isna() ].reset_index(drop=True)\n",
    "# remove outliers\n",
    "min_bathroom_s = 1\n",
    "min_bedroom_s = 1\n",
    "min_building_size_s = 6 # I guess 6 meters-squared is already a valid size\n",
    "min_land_size_s = 6 # The same with land size\n",
    "df = df[ df['data_bathrooms_s'] >= min_bathroom_s ]\n",
    "df = df[ df['data_bedrooms_s'] >= min_bedroom_s ]\n",
    "df = df[ df['data_building_size_s'] >= min_building_size_s ]\n",
    "df = df[ df['data_land_size_s'] >= min_land_size_s ]\n",
    "max_bathroom_s = np.percentile(df['data_bathrooms_s'], 99)\n",
    "max_bedroom_s = np.percentile(df['data_bedrooms_s'], 99)\n",
    "max_building_size_s = np.percentile(df['data_building_size_s'], 99)\n",
    "max_land_size_s = np.percentile(df['data_land_size_s'], 99)\n",
    "df = df[ df['data_bathrooms_s'] <= max_bathroom_s ]\n",
    "df = df[ df['data_bedrooms_s'] <= max_bedroom_s ]\n",
    "df = df[ df['data_building_size_s'] <= max_building_size_s ]\n",
    "df = df[ df['data_land_size_s'] <= max_land_size_s ]\n",
    "df = df.reset_index(drop=True)\n",
    "print('Number of rows after cleaning:', len(df))\n",
    "\n",
    "# create kabupaten and kecamatan columns\n",
    "kabupaten_pattern = r'([-\\w]+)/([-\\w]+)/house/buy/$'\n",
    "df['kabupaten'] = df['parent_link_s'].apply(lambda x: re.search(kabupaten_pattern, x)[1])\n",
    "kecamatan_pattern = r'/([-\\w]+)/house/buy/$'\n",
    "df['kecamatan'] = df['parent_link_s'].apply(lambda x: re.search(kecamatan_pattern, x)[1])\n",
    "\n",
    "# create CSV file from cleaned lamudi house dataset\n",
    "df.to_csv('lamudi_house_dataset_cleaned.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Scrape Google Maps for Facilities\n",
    "\n",
    "**Relevant Notebook File**: scrape_gmaps.ipynb\n",
    "\n",
    "**Output File**: facility_latlong.csv\n",
    "\n",
    "*Notes*: I skipped many manual patchings in this consolidated file to make it easier to read. Data from Google Maps in Indonesia is not as clean and we still need to patch the data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "# malls\n",
    "malls = []\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://id.wikipedia.org/wiki/Daftar_pusat_perbelanjaan_di_Jakarta\")\n",
    "mall_s = driver.find_elements(By.XPATH, '//*[@id=\"mw-content-text\"]/div[1]/ul/li')\n",
    "for mall in mall_s:\n",
    "    malls.append(mall.text)\n",
    "driver.quit()\n",
    "malls = [mall.replace('Mall', 'Mal').replace('Mal', 'Mall') for mall in malls]\n",
    "\n",
    "# train stations\n",
    "krls = []\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://id.wikipedia.org/wiki/Kategori:Stasiun_kereta_api_di_Jakarta\")\n",
    "krl_s = driver.find_elements(By.XPATH, '//*[@id=\"mw-pages\"]/div/div/div/ul/li/a')\n",
    "for krl in krl_s:\n",
    "    krls.append(krl.text)\n",
    "driver.quit()\n",
    "\n",
    "# transjakarta (bus)\n",
    "tjs = []\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://id.wikipedia.org/wiki/Daftar_koridor_Transjakarta\")\n",
    "tj_s = driver.find_elements(By.CSS_SELECTOR, 'tr>td:nth-child(2)>a.mw-redirect[href^=\"/wiki/\"]')\n",
    "for tj in tj_s:\n",
    "    tjs.append('Halte Busway ' + tj.text)\n",
    "driver.quit()\n",
    "tjs = sorted(set(tjs))\n",
    "\n",
    "# hospitals\n",
    "rss = []\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://id.wikipedia.org/wiki/Daftar_rumah_sakit_di_DKI_Jakarta\")\n",
    "rs_s = driver.find_elements(By.CSS_SELECTOR, 'tr>td:nth-child(3)>a.new[href]')\n",
    "for rs in rs_s:\n",
    "    rss.append(rs.text)\n",
    "driver.quit()\n",
    "\n",
    "list_to_scrape = malls + krls + tjs + rss\n",
    "len(list_to_scrape)\n",
    "\n",
    "url_s = ['']*len(list_to_scrape)\n",
    "stars = ['']*len(list_to_scrape)\n",
    "count_reviews = ['']*len(list_to_scrape)\n",
    "category = ['mall']*len(malls) + ['train_station']*len(krls) + ['bus_station']*len(tjs) + ['hospital']*len(rss)\n",
    "\n",
    "# scrape from google maps\n",
    "driver = webdriver.Chrome()\n",
    "for i in range(len(list_to_scrape)):\n",
    "    driver.get(\"https://www.google.com/maps/@-6.208428,106.7824432,15z\")\n",
    "    time.sleep(math.ceil(1.81 + np.random.normal(loc=0.0,scale=0.327), 0))\n",
    "    Place = driver.find_element(By.CLASS_NAME, \"tactile-searchbox-input\")\n",
    "    Place.send_keys(list_to_scrape[i])\n",
    "    time.sleep(math.ceil(1.81 + np.random.normal(loc=0.0,scale=0.327), 0))\n",
    "    Place.send_keys(Keys.DOWN)\n",
    "    time.sleep(0.15)\n",
    "    Submit = driver.find_element(By.ID, \"searchbox-searchbutton\")\n",
    "    Submit.click()\n",
    "    time.sleep(math.ceil(1.81 + np.random.normal(loc=0.0,scale=0.127), 0))\n",
    "    try:\n",
    "        url_s[i] = driver.current_url\n",
    "        stars[i] = driver.find_element(By.CSS_SELECTOR, 'span[aria-label^=\"Bintang\"]').accessible_name # bintang = star\n",
    "        count_reviews[i] = driver.find_element(By.CSS_SELECTOR, 'span[aria-label$=\"lasan\"]').accessible_name # ulasan = review\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(data = {\n",
    "    'name': list_to_scrape\n",
    "    , 'url': url_s\n",
    "    , 'stars': stars\n",
    "    , 'count_reviews': count_reviews\n",
    "    , 'category': category\n",
    "})\n",
    "\n",
    "# clean scraping data\n",
    "df['coordinate'] = df['url'].apply(lambda x: re.search(r'@([-\\d\\.,]+)z', x)[1])\n",
    "df['stars'] = df['stars'].apply(lambda x: re.search(r'^Bintang\\s([\\d,]+)', str(x))[1] if re.search(r'^Bintang\\s([\\d,]+)', str(x)) else None)\n",
    "df['count_reviews'] = df['count_reviews'].apply(lambda x: re.search(r'([\\d\\.]+)\\s.lasan$', str(x))[1] if re.search(r'([\\d\\.]+)\\s.lasan$', str(x)) else None)\n",
    "df['stars'] = df['stars'].str.replace(',', '.').astype('float64')\n",
    "df['stars'] = df['stars'].fillna(3)\n",
    "df['count_reviews'] = df['count_reviews'].fillna('5')\n",
    "df['count_reviews'] = df['count_reviews'].str.replace('.', '').astype(int)\n",
    "\n",
    "# split to get latitude and longitude\n",
    "df['latitude'] = df['coordinate'].apply(lambda x: x.split(',')[0])\n",
    "df['longitude'] = df['coordinate'].apply(lambda x: x.split(',')[1])\n",
    "\n",
    "# create facilities and latitude\n",
    "df.to_csv('facility_latlong.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Add Jakarta Flood Data from [Governmental Dashboard](https://public.tableau.com/app/profile/jsc.data/viz/DataPendukungPotensiGenangan/PetaAwal)\n",
    "\n",
    "**Input Files**:\n",
    "1. flood_0.png\n",
    "2. flood_1.png\n",
    "3. flood_2.png\n",
    "4. flood_3.png\n",
    "5. flood_4.png\n",
    "6. flood_5.png\n",
    "7. flood_6.png\n",
    "\n",
    "**Relevant Notebook File**: check_flood.ipynb\n",
    "\n",
    "*Notes*: Since the mapping from pixel to coordinate is tied to the pixel of the screen, it is better to just copy from my screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_pixel(lat, long):\n",
    "    return [ int(round(160+(478-160)*(long-(106.7351))/(106.91427-((106.7351))))), int(round(145+(654-145)*(lat-(-6.0808))/(-6.36549-((-6.0808))))) ]\n",
    "\n",
    "def get_flood_scenario(lat, long):\n",
    "    colors_list = []\n",
    "    pixel = get_pixel(lat, long)\n",
    "    flood_0 = 0\n",
    "    flood_1 = 0\n",
    "    flood_2 = 0\n",
    "    flood_3 = 0\n",
    "    flood_4 = 0\n",
    "    flood_5 = 0\n",
    "    flood_6 = 0\n",
    "    for i in range(7):\n",
    "        im = Image.open(f\"./flood_{i}.png\")\n",
    "        rgb_im = im.convert('RGB')\n",
    "        for h_px in range(-3,4):\n",
    "            for v_px in range(-3,4):\n",
    "                try:\n",
    "                    colors_list.append( rgb_im.getpixel((pixel[0]+v_px, pixel[1]+h_px)) )\n",
    "                except:\n",
    "                    pass\n",
    "        if i == 0 and (225, 87, 89) in colors_list:\n",
    "            flood_0 = 1\n",
    "        if i == 1 and (237, 201, 72) in colors_list:\n",
    "            flood_1 = 1\n",
    "        if i == 2 and (78, 121, 167) in colors_list:\n",
    "            flood_2 = 1\n",
    "        if i == 3 and (242, 142, 43) in colors_list:\n",
    "            flood_3 = 1\n",
    "        if i == 4 and (89, 161, 79) in colors_list:\n",
    "            flood_4 = 1\n",
    "        if i == 5 and (176, 122, 161) in colors_list:\n",
    "            flood_5 = 1\n",
    "        if i == 6 and (156, 117, 95) in colors_list:\n",
    "            flood_6 = 1\n",
    "    return [flood_0, flood_1, flood_2, flood_3, flood_4, flood_5, flood_6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Merge Data from Lamudi, NJOP, Flood Scenarios, and Facilities\n",
    "\n",
    "**Detailed inputs, notebooks, and outputs in the subsections**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Merge Data from Lamudi and NJOP\n",
    "\n",
    "**Input Files**:\n",
    "1. lamudi_house_dataset_cleaned.csv\n",
    "2. jakarta_street_coordinate_patched.csv\n",
    "\n",
    "**Relevant Notebook File**: EDA_NJOP.ipynb\n",
    "\n",
    "**Output File**: lamudi_njop_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.geocoders import Nominatim # https://nominatim.org/\n",
    "import time\n",
    "\n",
    "df_njop = pd.read_csv('NJOP2021.csv')\n",
    "njop_cols = ['kelurahan', 'nama_jalan', 'ave_njb']\n",
    "df_njop = df_njop[njop_cols]\n",
    "df_njop['kelurahan'] = df_njop['kelurahan'].apply(lambda x: str(x).split(' - ')[1])\n",
    "\n",
    "df = pd.read_csv('jakarta_street_coordinate_patched.csv')\n",
    "df_map = df[cols].merge(df_njop, how='inner', on=['kelurahan', 'nama_jalan'])\n",
    "df_map['ave_njb'] = df_map['ave_njb'].str.replace(',', '').astype('float')\n",
    "\n",
    "# https://stackoverflow.com/questions/1253499/simple-calculations-for-working-with-lat-lon-and-km-distance\n",
    "# Latitude: 1 deg = 110.574 km\n",
    "# Longitude: 1 deg = 111.320*cos(latitude) km\n",
    "def get_average_njop_near(lat, long, radius_km, df_map):\n",
    "    \"\"\"\n",
    "    Dependencies: numpy as np, pandas as pd, geopy.distance\n",
    "    Input: Latitude (float), Longitude (float), Radius in km (float)\n",
    "    Output: Average NJOP value based on df_map in the radius (float)\n",
    "    \"\"\"\n",
    "    d_lat = radius_km/110 # let's use upper bound of 110000\n",
    "    d_long = radius_km/111 # let's assume upper bound of cos(0) and 111000\n",
    "    df = df_map.copy(deep=True)\n",
    "    df = df[ (df['latitude'].between(lat-d_lat, lat+d_lat)) & (df['longitude'].between(long-d_long, long+d_long)) ] # bbox\n",
    "    try:\n",
    "        df['distance'] = df.apply(lambda x: geopy.distance.distance((lat, long) , (x['latitude'], x['longitude'])).km, axis=1)\n",
    "    except:\n",
    "        df['distance'] = np.nan\n",
    "    df = df[ df['distance'] <= radius_km ]\n",
    "    return [df['ave_njb'].mean(), df['ave_njb'].count(), df['ave_njb'].min(), df['ave_njb'].max(), df['ave_njb'].std()]\n",
    "\n",
    "df_lamudi = pd.read_csv('lamudi_house_dataset_cleaned.csv')\n",
    "df_lamudi = df_lamudi.drop('Unnamed: 0', axis=1) # from the creation of csv\n",
    "df_lamudi.head()\n",
    "\n",
    "geo_point_s = sorted(df_lamudi['data_geo_point_s'].astype('str').unique())\n",
    "print('Addresses count:', len(geo_point_s))\n",
    "average_njop = []\n",
    "neighbors = []\n",
    "counter = 0\n",
    "min_ave_njop = []\n",
    "max_ave_njop = []\n",
    "std_ave_njop = []\n",
    "for address in geo_point_s:\n",
    "    counter += 1\n",
    "    print('Now on address:', counter)\n",
    "    try:\n",
    "        address = eval(address)\n",
    "        sol = get_average_njop_near(address[1], address[0], 1, df_map)\n",
    "        average_njop.append(sol[0])\n",
    "        neighbors.append(sol[1])\n",
    "        min_ave_njop.append(sol[2])\n",
    "        max_ave_njop.append(sol[3])\n",
    "        std_ave_njop.append(sol[4])\n",
    "    except:\n",
    "        average_njop.append(np.nan)\n",
    "        neighbors.append(np.nan)\n",
    "        min_ave_njop.append(np.nan)\n",
    "        max_ave_njop.append(np.nan)\n",
    "        std_ave_njop.append(np.nan)\n",
    "# create new DataFrame of coordinate with NJOP\n",
    "df_lamudi_coord_njop = pd.DataFrame(data={'data_geo_point_s': geo_point_s, 'njop':average_njop, 'neighbors':neighbors\n",
    "                                          , 'min_njop': min_ave_njop, 'max_njop': max_ave_njop, 'std_njop': std_ave_njop})\n",
    "\n",
    "df_lamudi['data_geo_point_s'] = df_lamudi['data_geo_point_s'].astype('str')\n",
    "df_lamudi = df_lamudi.merge(df_lamudi_coord_njop, on='data_geo_point_s', how='left')\n",
    "df_lamudi = df_lamudi.drop('data_furnished_s', axis=1)\n",
    "df_lamudi = df_lamudi.dropna().reset_index(drop=True)\n",
    "\n",
    "df_lamudi.to_csv('lamudi_njop_dataset.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Merge Data to Flood Scenarios\n",
    "\n",
    "**Input Files**:\n",
    "1. lamudi_njop_dataset.csv\n",
    "2. flood_0.png\n",
    "3. flood_1.png\n",
    "4. flood_2.png\n",
    "5. flood_3.png\n",
    "6. flood_4.png\n",
    "7. flood_5.png\n",
    "8. flood_6.png \n",
    "\n",
    "**Relevant Notebook File**: check_flood.ipynb\n",
    "\n",
    "**Output File**: lamudi_njop_flood_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_pixel(lat, long):\n",
    "    return [ int(round(160+(478-160)*(long-(106.7351))/(106.91427-((106.7351))))), int(round(145+(654-145)*(lat-(-6.0808))/(-6.36549-((-6.0808))))) ]\n",
    "\n",
    "def get_flood_scenario(lat, long):\n",
    "    colors_list = []\n",
    "    pixel = get_pixel(lat, long)\n",
    "    flood_0 = 0\n",
    "    flood_1 = 0\n",
    "    flood_2 = 0\n",
    "    flood_3 = 0\n",
    "    flood_4 = 0\n",
    "    flood_5 = 0\n",
    "    flood_6 = 0\n",
    "    for i in range(7):\n",
    "        im = Image.open(f\"./flood_{i}.png\")\n",
    "        rgb_im = im.convert('RGB')\n",
    "        for h_px in range(-3,4):\n",
    "            for v_px in range(-3,4):\n",
    "                try:\n",
    "                    colors_list.append( rgb_im.getpixel((pixel[0]+v_px, pixel[1]+h_px)) )\n",
    "                except:\n",
    "                    pass\n",
    "        if i == 0 and (225, 87, 89) in colors_list:\n",
    "            flood_0 = 1\n",
    "        if i == 1 and (237, 201, 72) in colors_list:\n",
    "            flood_1 = 1\n",
    "        if i == 2 and (78, 121, 167) in colors_list:\n",
    "            flood_2 = 1\n",
    "        if i == 3 and (242, 142, 43) in colors_list:\n",
    "            flood_3 = 1\n",
    "        if i == 4 and (89, 161, 79) in colors_list:\n",
    "            flood_4 = 1\n",
    "        if i == 5 and (176, 122, 161) in colors_list:\n",
    "            flood_5 = 1\n",
    "        if i == 6 and (156, 117, 95) in colors_list:\n",
    "            flood_6 = 1\n",
    "    return [flood_0, flood_1, flood_2, flood_3, flood_4, flood_5, flood_6]\n",
    "\n",
    "df = pd.read_csv('lamudi_njop_dataset.csv')\n",
    "\n",
    "geo_point_s = sorted(df['data_geo_point_s'].astype('str').unique())\n",
    "print('Addresses count:', len(geo_point_s))\n",
    "counter = 0\n",
    "flood_cols = []\n",
    "for address in geo_point_s:\n",
    "    counter += 1\n",
    "    print('Now on address:', counter)\n",
    "    address = eval(address)\n",
    "    flood_cols.append(get_flood_scenario(address[1], address[0]))\n",
    "\n",
    "# create new DataFrame of coordinate with flood scenarios\n",
    "df_flood = pd.DataFrame(flood_cols, columns=['flood_0', 'flood_1', 'flood_2', 'flood_3', 'flood_4', 'flood_5', 'flood_6'])\n",
    "df_flood['data_geo_point_s'] = geo_point_s\n",
    "\n",
    "df['data_geo_point_s'] = df['data_geo_point_s'].astype('str')\n",
    "df = df.merge(df_flood, on='data_geo_point_s', how='left')\n",
    "\n",
    "df.to_csv('lamudi_njop_flood_dataset.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Merge Data to Facilities\n",
    "\n",
    "**Input Files**:\n",
    "1. lamudi_njop_flood_dataset.csv\n",
    "2. facility_latlong.csv\n",
    "\n",
    "**Relevant Notebook File**: check_facilities.ipynb\n",
    "\n",
    "**Output File**: final_dataset_unfiltered.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('facilities_latlong.csv')\n",
    "df_lamudi = pd.read_csv('lamudi_njop_flood_dataset.csv')\n",
    "\n",
    "# https://stackoverflow.com/questions/1253499/simple-calculations-for-working-with-lat-lon-and-km-distance\n",
    "# Latitude: 1 deg = 110.574 km\n",
    "# Longitude: 1 deg = 111.320*cos(latitude) km\n",
    "def get_facilities_near(lat, long, radius_km, df_map):\n",
    "    \"\"\"\n",
    "    Dependencies: numpy as np, pandas as pd, geopy.distance\n",
    "    Input: Latitude (float), Longitude (float), Radius in km (float)\n",
    "    Output: Columns of facility features, that is:\n",
    "        1. Count Mall facilities near lat and long\n",
    "        2. Max stars of the Mall facilities near lat and long\n",
    "        3. Sum of count reviews of the Mall facilities near lat and long\n",
    "        4. Count Hospital facilities near lat and long\n",
    "        5. Max stars of the Hospital facilities near lat and long\n",
    "        6. Sum of count reviews of the Hospital facilities near lat and long\n",
    "        7. Count Bus Station facilities near lat and long\n",
    "        9. Sum of count reviews of the Bus Station facilities near lat and long\n",
    "        8. Max stars of the Train Station facilities near lat and long\n",
    "        10. Sum of count reviews of the Train Station facilities near lat and long\n",
    "    \"\"\"\n",
    "    d_lat = radius_km/110 # let's use upper bound of 110000\n",
    "    d_long = radius_km/111 # let's assume upper bound of cos(0) and 111000\n",
    "    df = df_map.copy(deep=True)\n",
    "    df = df[ (df['latitude'].between(lat-d_lat, lat+d_lat)) & (df['longitude'].between(long-d_long, long+d_long)) ] # bbox\n",
    "    try:\n",
    "        df['distance'] = df.apply(lambda x: geopy.distance.distance((lat, long) , (x['latitude'], x['longitude'])).km, axis=1)\n",
    "    except:\n",
    "        df['distance'] = np.nan\n",
    "    df = df[ df['distance'] <= radius_km ]\n",
    "    return [\n",
    "        df[ df['category'] == 'mall']['name'].nunique()\n",
    "        , df[ df['category'] == 'mall']['stars'].max()\n",
    "        , df[ df['category'] == 'mall']['count_reviews'].sum()\n",
    "        , df[ df['category'] == 'hospital']['name'].nunique()\n",
    "        , df[ df['category'] == 'hospital']['stars'].max()\n",
    "        , df[ df['category'] == 'hospital']['count_reviews'].sum()\n",
    "        , df[ df['category'] == 'bus_station']['name'].nunique()\n",
    "        , df[ df['category'] == 'bus_station']['count_reviews'].sum()\n",
    "        , df[ df['category'] == 'train_station']['name'].nunique()\n",
    "        , df[ df['category'] == 'train_station']['count_reviews'].sum()\n",
    "    ]\n",
    "\n",
    "geo_point_s = sorted(df_lamudi['data_geo_point_s'].astype('str').unique())\n",
    "count_mall = []\n",
    "max_stars_mall = []\n",
    "sum_reviews_mall = []\n",
    "count_hospital = []\n",
    "max_stars_hospital = []\n",
    "sum_reviews_hospital = []\n",
    "count_bus_st = []\n",
    "sum_reviews_bus_st = []\n",
    "count_train_st = []\n",
    "sum_reviews_train_st = []\n",
    "df_map = df\n",
    "df_map[['latitude', 'longitude']] = df_map[['latitude', 'longitude']].astype('float')\n",
    "counter = 0\n",
    "for address in geo_point_s:\n",
    "    counter += 1\n",
    "    print('Now on address:', counter)\n",
    "    try:\n",
    "        address = eval(address)\n",
    "        sol = get_facilities_near(address[1], address[0], 2, df_map)\n",
    "        count_mall.append(sol[0])\n",
    "        max_stars_mall.append(sol[1])\n",
    "        sum_reviews_mall.append(sol[2])\n",
    "        count_hospital.append(sol[3])\n",
    "        max_stars_hospital.append(sol[4])\n",
    "        sum_reviews_hospital.append(sol[5])\n",
    "        count_bus_st.append(sol[6])\n",
    "        sum_reviews_bus_st.append(sol[7])\n",
    "        count_train_st.append(sol[8])\n",
    "        sum_reviews_train_st.append(sol[9])\n",
    "    except:\n",
    "        count_mall.append(np.nan)\n",
    "        max_stars_mall.append(np.nan)\n",
    "        sum_reviews_mall.append(np.nan)\n",
    "        count_hospital.append(np.nan)\n",
    "        max_stars_hospital.append(np.nan)\n",
    "        sum_reviews_hospital.append(np.nan)\n",
    "        count_bus_st.append(np.nan)\n",
    "        sum_reviews_bus_st.append(np.nan)\n",
    "        count_train_st.append(np.nan)\n",
    "        sum_reviews_train_st.append(np.nan)\n",
    "\n",
    "df_facilities = pd.DataFrame(data={'count_mall': count_mall\n",
    "                                    , 'max_stars_mall': max_stars_mall\n",
    "                                    , 'sum_reviews_mall': sum_reviews_mall\n",
    "                                    , 'count_hospital': count_hospital\n",
    "                                    , 'max_stars_hospital': max_stars_hospital\n",
    "                                    , 'sum_reviews_hospital': sum_reviews_hospital\n",
    "                                    , 'count_bus_st': count_bus_st\n",
    "                                    , 'sum_reviews_bus_st': sum_reviews_bus_st\n",
    "                                    , 'count_train_st': count_train_st\n",
    "                                    , 'sum_reviews_train_st': sum_reviews_train_st\n",
    "                                    , 'data_geo_point_s': geo_point_s})\n",
    "\n",
    "df_lamudi['data_geo_point_s'] = df_lamudi['data_geo_point_s'].astype('str')\n",
    "df_lamudi = df_lamudi.merge(df_facilities, on='data_geo_point_s', how='left')\n",
    "df_lamudi = df_lamudi.dropna().reset_index(drop=True)\n",
    "\n",
    "df_lamudi.to_csv('final_dataset_unfiltered.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering and Modelling\n",
    "\n",
    "**Input File**: final_dataset_unfiltered.csv\n",
    "\n",
    "**Relevant Notebook File**: feature_engineering_modelling.ipynb\n",
    "\n",
    "**Output File**: model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from math import ceil, floor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('final_dataset_unfiltered.csv')\n",
    "\n",
    "df['data_geo_point_s'] = df['data_geo_point_s'].astype('str')\n",
    "df['latitude'] = df['data_geo_point_s'].apply(lambda x: eval(x)[1] if x != 'nan' else None)\n",
    "df['longitude'] = df['data_geo_point_s'].apply(lambda x: eval(x)[0] if x != 'nan' else None)\n",
    "\n",
    "# remove kepulauan seribu\n",
    "df = df[ df['kabupaten'] != 'kepulauan-seribu' ].reset_index(drop=True)\n",
    "# remove wrong latitude and longitude\n",
    "df = df[ (df['latitude'].between(-7, -5)) & (df['longitude'].between(106, 108)) ].reset_index(drop=True)\n",
    "\n",
    "# create floor\n",
    "df['floor_by_size'] = np.vectorize(ceil)(df['data_building_size_s']/df['data_land_size_s'])\n",
    "floor_pattern = '(\\d+).?(lantai|lt)'\n",
    "df['floor_pattern'] = df['page_link_s'].apply(lambda x: re.search(floor_pattern, x)[1] if re.search(floor_pattern, x) else 0).astype(int)\n",
    "\n",
    "def get_floor(floor_pattern, floor_by_size):\n",
    "    if floor_pattern != 0 and floor_pattern < 5:\n",
    "        return floor_pattern\n",
    "    else:\n",
    "        return floor_by_size\n",
    "\n",
    "df['floor'] = np.vectorize(get_floor)(df['floor_pattern'], df['floor_by_size'])\n",
    "df = df.drop(['floor_pattern','floor_by_size'], axis=1)\n",
    "df[['max_stars_mall', 'max_stars_hospital']] = df[['max_stars_mall', 'max_stars_hospital']].fillna(0)\n",
    "\n",
    "model_columns = ['data_price_s', 'data_bedrooms_s', 'data_bathrooms_s', 'floor', 'data_land_size_s', 'data_building_size_s', 'njop'\n",
    "                 , 'neighbors', 'min_njop', 'max_njop', 'std_njop', 'flood_0', 'flood_1', 'flood_2', 'flood_3', 'flood_4'\n",
    "                 , 'flood_5', 'flood_6', 'count_mall', 'max_stars_mall', 'sum_reviews_mall', 'count_hospital', 'max_stars_hospital'\n",
    "                 , 'sum_reviews_hospital', 'count_bus_st', 'sum_reviews_bus_st', 'count_train_st', 'sum_reviews_train_st'\n",
    "                 , 'latitude', 'longitude', 'kabupaten', 'kecamatan']\n",
    "df = df[model_columns]\n",
    "\n",
    "# standardize kecamatan from links to the reality in the format of NJOP files\n",
    "dict_kecamatan = {\"ancol\": \"pademangan\", \"bendungan-hilir\": \"tanah-abang\", \"cempaka-putih-1\": \"cempaka-putih\"\n",
    "                  , \"cibubur\": \"ciracas\", \"cipayung-1\": \"cipayung\", \"jatinegara-1\": \"jatinegara\", \"jelambar\": \"grogol-petamburan\"\n",
    "                  , \"jatinegara-1\": \"jatinegara\", \"kemayoran-2\": \"kemayoran\", \"kuningan-2\": \"setiabudi\", \"setia-budi\": \"setiabudi\"\n",
    "                  , \"pantai-indah-kapuk\": \"penjaringan\", \"pluit\": \"penjaringan\", \"pondok-indah\": \"kebayoran-lama\"\n",
    "                  , \"pondok-kelapa-1\": \"duren-sawit\", \"puri-indah\": \"kembangan\", \"rawamangun\": \"pulo-gadung\"\n",
    "                  , \"sawah-besar-1\": \"sawah-besar\", \"tanah-sereal\": \"tambora\", \"tanjung-duren-utara\": \"grogol-petamburan\"\n",
    "                  , \"thamrin\": \"menteng\"}\n",
    "df['kecamatan'] = df['kecamatan'].apply(lambda x: dict_kecamatan[x] if x in dict_kecamatan else x).apply(lambda x: str(x).upper().replace('-', ' '))\n",
    "\n",
    "# clean price_per_size outliers\n",
    "df = df[\n",
    "    ((df['data_price_s']/(df['data_land_size_s']+df['data_building_size_s']))/df['njop'])\\\n",
    "    .between(np.percentile(((df['data_price_s']/(df['data_land_size_s']+df['data_building_size_s']))/df['njop']), 1)\n",
    "    , np.percentile(((df['data_price_s']/(df['data_land_size_s']+df['data_building_size_s']))/df['njop']), 99))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# create model\n",
    "df['price_per_size'] = df['data_price_s']/(df['data_land_size_s']+df['data_building_size_s'])\n",
    "df = df.drop('data_price_s', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop('price_per_size', axis=1)\n",
    "    , df['price_per_size']\n",
    "    , test_size=0.3\n",
    "    , random_state=42\n",
    ")\n",
    "pipe = Pipeline([\n",
    "    ('categorical_encoder'\n",
    "     , OrdinalEncoder(encoding_method='ordered'\n",
    "                      , variables=['kota', 'kecamatan']))\n",
    "    , ('xgbr', XGBRegressor(random_state=42))\n",
    "])\n",
    "param_grid = {\n",
    "    'xgbr__learning_rate': [0.2]\n",
    "    , 'xgbr__n_estimators': [120]\n",
    "    , 'xgbr__max_depth': [6]\n",
    "    , 'xgbr__min_child_weight': [1]\n",
    "    , 'xgbr__gamma': [0.5]\n",
    "    , 'xgbr__reg_alpha': [0.01]\n",
    "    , 'xgbr__reg_lambda': [5]\n",
    "}\n",
    "grid_search_tune = GridSearchCV(pipe, param_grid\n",
    "                           , cv=5, n_jobs=-1, scoring='neg_mean_absolute_percentage_error')\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "print(\"Negative Mean Absolute Error Train:\", grid_search_tune.score(X_train, y_train))\n",
    "print(\"Negative Mean Absolute Error Test:\", grid_search_tune.score(X_test, y_test))\n",
    "\n",
    "# train model with all data\n",
    "X = df.drop('price_per_size', axis=1)\n",
    "y = df['price_per_size']\n",
    "grid_search_tune.fit(X, y)\n",
    "\n",
    "# store model\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search_tune, f)\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Preparation for Web Apps\n",
    "\n",
    "**Input**: facility_latlong.csv\n",
    "\n",
    "**Relevant Notebook File**: user_inputs.ipynb\n",
    "\n",
    "**Output**: kota_kecamatan_kelurahan_jalan_latlong.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_njop = pd.read_csv('facility_latlong.csv')\n",
    "df_njop = df_njop[['kota','kecamatan', 'kelurahan', 'nama_jalan', 'latitude', 'longitude']]\n",
    "df_njop.to_csv('kota_kecamatan_kelurahan_jalan_latlong.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_price_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
